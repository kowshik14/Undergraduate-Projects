{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf382896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, welch\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29acd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading .arff files\n",
    "from scipy.io import arff\n",
    "\n",
    "h_train_data, h_train_meta = arff.loadarff('../data/EOGHorizontalSignal_TRAIN.arff')\n",
    "hEOG_train = pd.DataFrame(h_train_data)\n",
    "\n",
    "h_test_data, h_test_meta = arff.loadarff('../data/EOGHorizontalSignal_TEST.arff')\n",
    "hEOG_test = pd.DataFrame(h_test_data)\n",
    "\n",
    "v_train_data, v_train_meta = arff.loadarff('../data/EOGVerticalSignal_TRAIN.arff')\n",
    "vEOG_train = pd.DataFrame(v_train_data)\n",
    "\n",
    "v_test_data, v_test_meta = arff.loadarff('../data/EOGVerticalSignal_TEST.arff')\n",
    "vEOG_test = pd.DataFrame(v_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67935c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert target column into integer\n",
    "hEOG_train['target'] = hEOG_train['target'].apply(lambda x: int(x.decode('utf-8')))\n",
    "vEOG_train['target'] = vEOG_train['target'].apply(lambda x: int(x.decode('utf-8')))\n",
    "hEOG_test['target'] = hEOG_test['target'].apply(lambda x: int(x.decode('utf-8')))\n",
    "vEOG_test['target'] = vEOG_test['target'].apply(lambda x: int(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49755fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming both have the same target\n",
    "y_train = hEOG_train['target'].copy()\n",
    "y_test = hEOG_test['target'].copy()\n",
    "\n",
    "# Optionally, concatenate the horizontal and vertical data along columns (axis=1)\n",
    "X_train_combined = pd.concat([hEOG_train.drop(columns=['target']), vEOG_train.drop(columns=['target'])], axis=1)\n",
    "X_test_combined = pd.concat([hEOG_test.drop(columns=['target']), vEOG_test.drop(columns=['target'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73cf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal de-noising\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Parameters for filtering\n",
    "fs = 1000  # Sampling rate (1 kHz)\n",
    "lowcut = 0.05  # Low frequency cut (for removing drift)\n",
    "highcut = 30.0  # High frequency cut (for noise reduction)\n",
    "\n",
    "# Apply filtering on the train EOG signal\n",
    "hEOG_train_filtered = hEOG_train.drop(columns=['target']).apply(lambda x: butter_bandpass_filter(x, lowcut, highcut, fs))\n",
    "vEOG_train_filtered = vEOG_train.drop(columns=['target']).apply(lambda x: butter_bandpass_filter(x, lowcut, highcut, fs))\n",
    "# Apply filtering on the test EOG signal\n",
    "hEOG_test_filtered = hEOG_test.drop(columns=['target']).apply(lambda x: butter_bandpass_filter(x, lowcut, highcut, fs))\n",
    "vEOG_test_filtered = vEOG_test.drop(columns=['target']).apply(lambda x: butter_bandpass_filter(x, lowcut, highcut, fs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6d3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract time domain & statistical features\n",
    "\n",
    "def extract_time_domain_features(signal):\n",
    "    features = {\n",
    "        'signal_variance': np.var(signal),\n",
    "        'signal_iqr': np.percentile(signal, 75) - np.percentile(signal, 25),\n",
    "        'mean': np.mean(signal),\n",
    "        'std': np.std(signal),\n",
    "        'max': np.max(signal),\n",
    "        'min': np.min(signal),\n",
    "        'skew': pd.Series(signal).skew(),\n",
    "        'kurtosis': pd.Series(signal).kurtosis(),\n",
    "        'energy': np.sum(np.square(signal)),\n",
    "        'median': np.median(signal),\n",
    "        'entropy':entropy(np.abs(signal))\n",
    "    }\n",
    "    \n",
    "    # Calculate the slope of the signal\n",
    "    if len(signal) > 1:\n",
    "        signal_slope = np.diff(signal)  # First derivative (slope)\n",
    "        features['mean_slope'] = np.mean(signal_slope)\n",
    "        features['std_slope'] = np.std(signal_slope)\n",
    "    else:\n",
    "        features['mean_slope'] = 0\n",
    "        features['std_slope'] = 0\n",
    "        \n",
    "        \n",
    "    return pd.Series(features)\n",
    "\n",
    "# Apply feature extraction to each row of the dataset\n",
    "hEOG_train_time_features = hEOG_train_filtered.apply(extract_time_domain_features, axis=1)\n",
    "vEOG_train_time_features = vEOG_train_filtered.apply(extract_time_domain_features, axis=1)\n",
    "hEOG_test_time_features = hEOG_test_filtered.apply(extract_time_domain_features, axis=1)\n",
    "vEOG_test_time_features = vEOG_test_filtered.apply(extract_time_domain_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b209069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time Features Columns:\n",
      "Index(['h_signal_variance', 'h_signal_iqr', 'h_mean', 'h_std', 'h_max',\n",
      "       'h_min', 'h_skew', 'h_kurtosis', 'h_energy', 'h_median', 'h_entropy',\n",
      "       'h_mean_slope', 'h_std_slope', 'v_signal_variance', 'v_signal_iqr',\n",
      "       'v_mean', 'v_std', 'v_max', 'v_min', 'v_skew', 'v_kurtosis', 'v_energy',\n",
      "       'v_median', 'v_entropy', 'v_mean_slope', 'v_std_slope'],\n",
      "      dtype='object')\n",
      "\n",
      "Test Time Features Columns:\n",
      "Index(['h_signal_variance', 'h_signal_iqr', 'h_mean', 'h_std', 'h_max',\n",
      "       'h_min', 'h_skew', 'h_kurtosis', 'h_energy', 'h_median', 'h_entropy',\n",
      "       'h_mean_slope', 'h_std_slope', 'v_signal_variance', 'v_signal_iqr',\n",
      "       'v_mean', 'v_std', 'v_max', 'v_min', 'v_skew', 'v_kurtosis', 'v_energy',\n",
      "       'v_median', 'v_entropy', 'v_mean_slope', 'v_std_slope'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Rename columns in horizontal features\n",
    "hEOG_train_time_features.columns = ['h_' + col for col in hEOG_train_time_features.columns]\n",
    "hEOG_test_time_features.columns = ['h_' + col for col in hEOG_test_time_features.columns]\n",
    "\n",
    "# Rename columns in vertical features\n",
    "vEOG_train_time_features.columns = ['v_' + col for col in vEOG_train_time_features.columns]\n",
    "vEOG_test_time_features.columns = ['v_' + col for col in vEOG_test_time_features.columns]\n",
    "\n",
    "# Combine horizontal and vertical features\n",
    "combined_train_time_features = pd.concat([hEOG_train_time_features, vEOG_train_time_features], axis=1)\n",
    "combined_test_time_features = pd.concat([hEOG_test_time_features, vEOG_test_time_features], axis=1)\n",
    "\n",
    "# Display the new column names\n",
    "print(\"Training Time Features Columns:\")\n",
    "print(combined_train_time_features.columns)\n",
    "\n",
    "print(\"\\nTest Time Features Columns:\")\n",
    "print(combined_test_time_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d21b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract frequency domain features\n",
    "from scipy.fft import fft\n",
    "\n",
    "def extract_frequency_domain_features(signal):\n",
    "    # Ensure that signal is a 1D numpy array\n",
    "    signal = np.array(signal).flatten()\n",
    "    \n",
    "    # Compute FFT\n",
    "    fft_vals = fft(signal)\n",
    "    \n",
    "    # Compute magnitude and power\n",
    "    fft_magnitude = np.abs(fft_vals)\n",
    "    power = np.square(fft_magnitude)\n",
    "    \n",
    "    # Extract frequency domain features\n",
    "    features = {\n",
    "        'fft_max_freq': np.argmax(fft_magnitude),\n",
    "        'fft_mean_power': np.mean(power),\n",
    "        'fft_peak_power': np.max(power),\n",
    "    }\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Apply FFT feature extraction to each row of the dataset\n",
    "hEOG_train_freq_features = hEOG_train_filtered.apply(lambda row: extract_frequency_domain_features(row.values), axis=1)\n",
    "vEOG_train_freq_features = vEOG_train_filtered.apply(lambda row: extract_frequency_domain_features(row.values), axis=1)\n",
    "\n",
    "hEOG_test_freq_features = hEOG_test_filtered.apply(lambda row: extract_frequency_domain_features(row.values), axis=1)\n",
    "vEOG_test_freq_features = vEOG_test_filtered.apply(lambda row: extract_frequency_domain_features(row.values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0ae9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Frequency Features Columns:\n",
      "Index(['h_fft_max_freq', 'h_fft_mean_power', 'h_fft_peak_power',\n",
      "       'v_fft_max_freq', 'v_fft_mean_power', 'v_fft_peak_power'],\n",
      "      dtype='object')\n",
      "\n",
      "Test Frequency Features Columns:\n",
      "Index(['h_fft_max_freq', 'h_fft_mean_power', 'h_fft_peak_power',\n",
      "       'v_fft_max_freq', 'v_fft_mean_power', 'v_fft_peak_power'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Rename columns in horizontal frequency features\n",
    "hEOG_train_freq_features.columns = ['h_' + col for col in hEOG_train_freq_features.columns]\n",
    "hEOG_test_freq_features.columns = ['h_' + col for col in hEOG_test_freq_features.columns]\n",
    "\n",
    "# Rename columns in vertical frequency features\n",
    "vEOG_train_freq_features.columns = ['v_' + col for col in vEOG_train_freq_features.columns]\n",
    "vEOG_test_freq_features.columns = ['v_' + col for col in vEOG_test_freq_features.columns]\n",
    "\n",
    "# Combine horizontal and vertical frequency domain features\n",
    "combined_train_freq_features = pd.concat([hEOG_train_freq_features, vEOG_train_freq_features], axis=1)\n",
    "combined_test_freq_features = pd.concat([hEOG_test_freq_features, vEOG_test_freq_features], axis=1)\n",
    "\n",
    "# Display the new column names\n",
    "print(\"Training Frequency Features Columns:\")\n",
    "print(combined_train_freq_features.columns)\n",
    "\n",
    "print(\"\\nTest Frequency Features Columns:\")\n",
    "print(combined_test_freq_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f13517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Training Features Shape: (362, 32)\n",
      "Final Combined Test Features Shape: (362, 32)\n"
     ]
    }
   ],
   "source": [
    "# Combine the time domain and frequency domain features for the training set\n",
    "combined_train_features = pd.concat([combined_train_time_features, combined_train_freq_features], axis=1)\n",
    "\n",
    "# Similarly, combine the test set features\n",
    "combined_test_features = pd.concat([combined_test_time_features, combined_test_freq_features], axis=1)\n",
    "\n",
    "# Display the shae of the final combined training features\n",
    "print(\"Final Combined Training Features Shape:\", combined_train_features.shape)\n",
    "\n",
    "# Display the shape of the final combined test features\n",
    "print(\"Final Combined Test Features Shape:\", combined_test_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03aacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)  # n_estimators is the number of trees\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(combined_train_features, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(combined_test_features)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report for detailed metrics\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix to see detailed performance\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47284994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'final_combined_train_features' and 'final_combined_test_features' are your feature DataFrames\n",
    "X_train = combined_train_features.values\n",
    "X_test = combined_test_features.values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data to 3D for GRU input: (samples, timesteps, features)\n",
    "# Here, timesteps = 1, and features = number of features\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "# Assuming you have 'y_train' and 'y_test' as target labels, convert them to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d80c6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9563938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to fit Conv1D and GRU layers\n",
    "X_train_combined_reshaped = X_train_combined.values.reshape(X_train_combined.values.shape[0], 1, X_train_combined.values.shape[1])\n",
    "X_test_combined_reshaped = X_test_combined.values.reshape(X_test_combined.values.shape[0], 1, X_test_combined.values.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69efea31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 1, 2500)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b83fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_5 (Conv1D)           (None, 1, 64)             320064    \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 1, 32)             4128      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 1, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 1, 128)           37632     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 128)            0         \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 32)                15552     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 12)                204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 378,108\n",
      "Trainable params: 378,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Bidirectional, GRU, Dense, Dropout\n",
    "\n",
    "# Define the CNN-GRU model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a 1D Convolutional Layer with 'same' padding to preserve input size\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', padding='same', input_shape=(1, 2500)))\n",
    "\n",
    "# Another Convolutional Layer with 'same' padding\n",
    "model.add(Conv1D(filters=32, kernel_size=2, activation='relu', padding='same'))\n",
    "\n",
    "# Optional: Add MaxPooling, but keep pool size as 1 to avoid further downsampling\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "\n",
    "# Add a Bidirectional GRU layer\n",
    "model.add(Bidirectional(GRU(units=64, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add another GRU layer\n",
    "model.add(GRU(units=32))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Flatten the output before Dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a Dense layer for additional learning\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# Output layer for multi-class classification\n",
    "model.add(Dense(units=12, activation='softmax'))  # Assuming 12 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17054d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust labels to be zero-indexed\n",
    "y_train_zero_indexed = y_train - 1  # Subtract 1 from each label in training set\n",
    "y_test_zero_indexed = y_test - 1      # Subtract 1 from each label in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c3f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_combined_reshaped, y_train_zero_indexed,\n",
    "                    epochs=10,                # You can adjust the number of epochs\n",
    "                    batch_size=32,             # Batch size\n",
    "                    validation_data=(X_test_combined_reshaped, y_test_zero_indexed),  # Validation set\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25ec04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
